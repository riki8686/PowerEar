# PowerEar

PowerEar is an acoustic eavesdropping attack that leverages the power side channel to reconstruct any audio reproduced by the built-in speaker of a mobile device, with a non-constrained vocabulary.

PowerEar can successfully recover user speeches from power consumption data in realistic settings, including speech utterances of individuals and different device models, mobile operating systems, activities, charging technology, battery, and volume level.

## Code structure
- Our code is based on the BicycleGAN model code "https://github.com/junyanz/BicycleGAN", which we re-adapt for the domain of this project.
- The `data` folder includes all modules related to data loading and preprocessing
- The `model` folder contains modules related to objective functions, optimizations, and neural network architectures.
- The `util` folder includes the html generator and visualizer utils.
- The `options` folder includes the configuration files where one can set the hyperparameters for the model training and testing
- The `scripts` folder has the scripts to execute the training and test processes.
- The `requirements.txt` contains the list of the Python libraries that one has to install.

## Getting Started 

______________

### Power traces collection

- Setup the mobile device under test for power trace measurements.

- Use `DAQExpress` to collect data. 

- Wait until the `Simulation` tag the `Analog Input` module disappears, then click the `Analog Input` module, pick `ai1` channel. The sampling rate parameter setting for experiments is 32000Hz, then start data recording.

- Save the `.csv` data to  `./audioSet`

  

### Data processing 

- Run `power2wav.py` to import the collected power consumption data, perform signal processing and then convert data to  `xxx.wav`  file.

- Use Adobe Audition to make sure that the original audio and the power trace are aligned.

- Use `crop2cGan` to cut the aligned audio files for training and test sets and place them in the folder "datasets".

### Model training

- Use `crop2cGan.py`, this script automatically slices and concatenates the audio to generate training and test sets.

- Train a model

  ```bash
  ./scripts/train.sh
  ```

- Test the model

  ```bash
  ./scripts/test.sh
  ```

### Audio reconstruction from spectrograms

- After running the test code, one can run the `griffin_lim.py` to get the audio file for every restored spectrogram in the test results.
- `X_ground truth.png` is a spectrogram of the original audio, `X_input.png` is input spectrogram (power trace) to the model, `X_encoded.png` is the audio-reconstructed spectrogram generated by the model, 


  

### MCD Metric

- Use the `MCD.py` script to calculate audio similarity, Input two `.WAV` format audio files and output the MCD value.


### Cross-Device Testing

- Execute the cross-device test command (e.g., taking "sony" testing with device-specific model trained with "oppo" data), the output files are saved to the `./results_cross/xxxx` directory.

  you can modify the script of `test.sh` like this
  ```BASH
  python test.py --dataroot ./testdata/sony --name oppoTotal_data_bicycle_gan --checkpoints_dir checkpoints --results_dir results_cross/sony/oppo --model bicycle_gan --direction BtoA
  ```
### Ablation study

- When you run the `power2wav.py`, you can set whether to apply or not a filter for signal processing. Using them to train and test one can get the ablation results.


  